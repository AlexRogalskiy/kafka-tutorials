////
In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.
The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.
////

To complete this tutorial, you'll build an application that uses a `KafkaProducer` and  `KafkaConsumer` instance for producing both Avro and Protobuf.  The approach you'll take in this tutorial is not typicaly of applications you'll build in a production setting. But by using multiple clients you can compare how to handle multiple event types
for each serialzer format.

Let's go over some of the key parts of the `KafkaMultiEventConsumerApplication` starting with the producer for the Protobuf events.

NOTE: Since this an advanced topic, the tutorial doesn't go into the basics of using a `KafkaProducer`. For more details see the https://creating-first-apache-kafka-producer-application/confluent.html[KafkaProducer tutorial]

[source, java]
.KafkaProducer for Protobuf events
----
public void produceProtobufEvents(final Supplier<Producer<String, CustomerEventProto.CustomerEvent>> producerSupplier,
                                  final String topic,
                                  final List<CustomerEventProto.CustomerEvent> protoCustomerEvents) {

        try (Producer<String, CustomerEventProto.CustomerEvent> producer = producerSupplier.get()) { <1>
            protoCustomerEvents.stream()    <2>
                    .map((event -> new ProducerRecord<>(topic, event.getId(), event))) <3>
                    .forEach(producerRecord -> producer.send(producerRecord, ((metadata, exception)

      //Details left out for clarity

   // Relevant configurations

 protoProduceConfigs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaProtobufSerializer.class);
}
----

<1> Retrieving the producer instance from the `Supplier`
<2> Using a `java.util.stream` to map each event into  a `ProducerRecord` then send them to the broker
<3> Creating the `ProducerRecord` instance.

There are two points to emphasize here.  The first is the type of the producer, its using `CustomerEventProto.CustomerEvent` since you must use an outer class with Protobuf, the generics on the producer are a concrete type.  As a result, to set the key to be the customer id you can call the `CustomerEvent#getId` method directly.  Note the use of the `Supplier` to provide the producer this is done to delay the instantiation until the `Supplier.get()` method is executed.  Using a supplier also makes testing easier by simplifying the process of providing a different implementation.

The second point is that you can use auto-registration feature of Scheama Registry with Protobuf and the referenced schemas get registered recursively.

Let's take a look at the Avro producer code now:
[source, java]
.KafkaProducer for Avro events
----
public void produceAvroEvents(final Supplier<Producer<String, SpecificRecordBase>> producerSupplier,
                              final String topic,
                              final List<SpecificRecordBase> avroEvents) {

        try (Producer<String, SpecificRecordBase> producer = producerSupplier.get()) {  <1>
           avroEvents.stream()  <2>
                    .map((event -> new ProducerRecord<>(topic, (String) event.get("customer_id"), event))) <3>
                    .forEach(producerRecord -> producer.send(producerRecord,
//Details left out for clarity

//Relevant configurations

 avroProduceConfigs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
 avroProduceConfigs.put(AbstractKafkaSchemaSerDeConfig.AUTO_REGISTER_SCHEMAS, false); <4>
 avroProduceConfigs.put(AbstractKafkaSchemaSerDeConfig.USE_LATEST_VERSION, true) <5>
----

<1> Getting the producer from the supplier
<2> Streaming over the provided collection of Avro events to send
<3> Creating the `ProducerRecord` instance, note the use of map-like access to get the required field for the key
<4> Specifying **_to disable_** automatic schema registation
<5> Setting to use the latest schema

You have a very similar approach with the Avro producer, but take a took at the type at annotation one - it's an abstract class, `SpecificRecordBase` that every Avro generated class inherits from.  Since the schema for the Avro multi-event topic uses a `union` at the top level, you don't know the concrete type.  Since you want to use the customer id as the key you need to access the field in a map-like fashion by using the field name as it exists in the schema.  This is possible because `SpecificRecordBase` implements the `GenericRecord` interface which provides the `get` method for retrieving a field value by name.

But the biggest difference is the configurations you provide to the producer for the Avro serializer. namely disabling automatic schema registration, otherwise it would override the union schema as the latest one.  Additionally since you've set `use.latest.version` to `true` the serializer looks up the latest version, the union schema, and will use that for serialization. https://www.confluent.io/blog/multiple-event-types-in-the-same-kafka-topic/#avro-unions-with-schema-references[This blog post] by Robert Yokota explains this mechanism in detail.

Next let's move on to the `KafkaConsumer` instances, starting with the Protobuf version.

[source, java]
.KafkaConsumer for multiple events
----
   consumerRecords.forEach(consumerRec -> {
    CustomerEventProto.CustomerEvent customerEvent = consumerRec.value();
    switch (customerEvent.getActionCase()) { <1>
        case PURCHASE:
            eventTracker.add(customerEvent.getPurchase().getItem()); <2>
            break;
        case PAGE_VIEW:
            eventTracker.add(customerEvent.getPageView().getUrl()); <3>
            break;

// details left out for clarity
----

<1> Using a `switch` statement for the different `enum` types
<2> Adding the purchased item to the event tracker
<3> Adding the page view link to the event tracker

With Protobuf when you have a `oneof` field, it generates an `enum` for each message that could possibly be in the field.  Determining which record to work with is straight forward by using different code blocks for each case.  The method to retrieve the `enum` alwasy follows the naming convention get<FieldName>Case.  So in this case the field name is `action` you use `getActionCase` for determining the type in the field.

Now lets take a look at the Avro consumer.
[source, java]
.KafkaConsumer for multiple events in Avro
----
consumerRecords.forEach(consumerRec -> {
  SpecificRecord avroRecord = consumerRec.value(); <1>
  if (avroRecord instanceof Purchase) {    <2>
      Purchase purchase = (Purchase) avroRecord;  <3>
      eventTracker.add(purchase.getItem());
  } else if (avroRecord instanceof PageView) {
      PageView pageView = (PageView) avroRecord;
      eventTracker.add(pageView.getUrl());
----

<1> Getting the record
<2> Doing an `instanceof` check to detrmine the type
<3> Casting to the appropriate concrete type

With the Avro consumer you'll need to use the Java `instanceof` operator to determine concrete type for the record.  Notice that here you're using the `SpecificRecord` interface which every Avro generated object implements.  Once you find the correct concrete type you cast the record to that type and extract the required inforation.

Now go ahead and create the `src/main/java/io/confluent/developer/KafkaMultiEventConsumerApplication.java` file:

+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/multiple-event-types/kafka/code/src/main/java/io/confluent/developer/KafkaMultiEventConsumerApplication.java %}</code></pre>
+++++
