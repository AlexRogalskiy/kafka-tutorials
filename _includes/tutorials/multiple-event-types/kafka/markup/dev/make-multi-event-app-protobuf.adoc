////
In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.
The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.
////

To complete this tutorial, you'll build an application that uses a `KafkaProducer` and  `KafkaConsumer` instance for producing both Avro and Protobuf.  The approach you'll take in this tutorial is not typicaly of applications you'll build in a production setting. But by using multiple clients you can compare how to handle multiple event types for each serialzer format.

**To that end, the point of this sample application is this:** you want capture pageview and purchase events in the exact order that they occur and you feel the best option is to have these events produced to the same topic.  Since the customer id will be the message key, you are guaranteed to get per-customer events in the order that they occur.



Let's go over some of the key parts of the `KafkaMultiEventConsumerApplication` starting with the producer for the Protobuf events.

NOTE: Since this an advanced topic, the tutorial doesn't go into the basics of using a `KafkaProducer`. For more details see the https://creating-first-apache-kafka-producer-application/confluent.html[KafkaProducer tutorial]

[source, java]
.KafkaProducer for Protobuf events
----
public void produceProtobufEvents(final Supplier<Producer<String, CustomerEventProto.CustomerEvent>> producerSupplier,
                                  final String topic,
                                  final List<CustomerEventProto.CustomerEvent> protoCustomerEvents) {

        try (Producer<String, CustomerEventProto.CustomerEvent> producer = producerSupplier.get()) { <1>
            protoCustomerEvents.stream()    <2>
                    .map((event -> new ProducerRecord<>(topic, event.getId(), event))) <3>
                    .forEach(producerRecord -> producer.send(producerRecord, ((metadata, exception)

      //Details left out for clarity

   // Relevant configurations

 protoProduceConfigs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaProtobufSerializer.class);
}
----

<1> Retrieving the producer instance from the `Supplier`
<2> Using a `java.util.stream` to map each event into  a `ProducerRecord` then send them to the broker
<3> Creating the `ProducerRecord` instance.

There are two points to emphasize here.  The first is the type of the producer, it's using `CustomerEventProto.CustomerEvent` since you must use an outer class with Protobuf, the generics on the producer are a concrete type.  As a result, to set the key to be the customer id you can call the `CustomerEvent#getId` method directly.  Note the use of the `Supplier` to provide the producer this is done to delay the instantiation until the `Supplier.get()` method is executed.  Using a supplier also makes testing easier by simplifying the process of providing a different implementation.

The second point is that you can use auto-registration feature of Scheama Registry with Protobuf and the referenced schemas get registered recursively.

Next let's move on to the `KafkaConsumer` for the Protobuf application.

[source, java]
.KafkaConsumer for multiple events in Protobuf
----
   consumerRecords.forEach(consumerRec -> {
    CustomerEventProto.CustomerEvent customerEvent = consumerRec.value();
    switch (customerEvent.getActionCase()) { <1>
        case PURCHASE:
            eventTracker.add(customerEvent.getPurchase().getItem()); <2>
            break;
        case PAGE_VIEW:
            eventTracker.add(customerEvent.getPageView().getUrl()); <3>
            break;



// details left out for clarity
----

<1> Using a `switch` statement for the different `enum` types
<2> Adding the purchased item to the event tracker
<3> Adding the page view link to the event tracker

With Protobuf when you have a `oneof` field, it generates an `enum` for each message that could possibly be in the field.  Determining which record to work with is straight forward by using different code blocks for each case.  The method to retrieve the `enum` alwasy follows the naming convention get<FieldName>Case.  So in this case the field name is `action` you use `getActionCase` for determining the type in the field.

